{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(D):\n",
    "    \"\"\"\n",
    "    Given a list of documents, where each document is represented as\n",
    "    a list of tokens, return the resulting vocabulary. The vocabulary\n",
    "    should be a set of tokens which appear more than once in the entire\n",
    "    document collection plus the \"<unk>\" token.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBoWFeaturizer(object):\n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \"\"\"\n",
    "        Given a document represented as a list of tokens and the vocabulary\n",
    "        as a set of tokens, compute the binary bag-of-words feature representation.\n",
    "        This function should return a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoWFeaturizer(object):\n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \"\"\"\n",
    "        Given a document represented as a list of tokens and the vocabulary\n",
    "        as a set of tokens, compute the count bag-of-words feature representation.\n",
    "        This function should return a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(D, vocab):\n",
    "    \"\"\"\n",
    "    Given a list of documents D and the vocabulary as a set of tokens,\n",
    "    where each document is represented as a list of tokens, return the IDF scores\n",
    "    for every token in the vocab. The IDFs should be represented as a dictionary that\n",
    "    maps from the token to the IDF value. If a token is not present in the\n",
    "    vocab, it should be mapped to \"<unk>\".\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "    \n",
    "class TFIDFFeaturizer(object):\n",
    "    def __init__(self, idf):\n",
    "        \"\"\"The idf scores computed via `compute_idf`.\"\"\"\n",
    "        self.idf = idf\n",
    "    \n",
    "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
    "        \"\"\"\n",
    "        Given a document represented as a list of tokens and\n",
    "        the vocabulary as a set of tokens, compute\n",
    "        the TF-IDF feature representation. This function\n",
    "        should return a dictionary which maps from the name of the\n",
    "        feature to the value of that feature.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should not need to edit this cell\n",
    "def load_dataset(file_path):\n",
    "    D = []\n",
    "    y = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            instance = json.loads(line)\n",
    "            D.append(instance['document'])\n",
    "            y.append(instance['label'])\n",
    "    return D, y\n",
    "\n",
    "def convert_to_features(D, featurizer, vocab):\n",
    "    X = []\n",
    "    for doc in D:\n",
    "        X.append(featurizer.convert_document_to_feature_dictionary(doc, vocab))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X, y, k, vocab):\n",
    "    \"\"\"\n",
    "    Computes the statistics for the Naive Bayes classifier.\n",
    "    X is a list of feature representations, where each representation\n",
    "    is a dictionary that maps from the feature name to the value.\n",
    "    y is a list of integers that represent the labels.\n",
    "    k is a float which is the smoothing parameters.\n",
    "    vocab is the set of vocabulary tokens.\n",
    "    \n",
    "    Returns two values:\n",
    "        p_y: A dictionary from the label to the corresponding p(y) score\n",
    "        p_v_y: A nested dictionary where the outer dictionary's key is\n",
    "            the label and the innner dictionary maps from a feature\n",
    "            to the probability p(v|y). For example, `p_v_y[1][\"hello\"]`\n",
    "            should be p(v=\"hello\"|y=1).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(D, p_y, p_v_y):\n",
    "    \"\"\"\n",
    "    Runs the prediction rule for Naive Bayes. D is a list of documents,\n",
    "    where each document is a list of tokens.\n",
    "    p_y and p_v_y are output from `train_naive_bayes`.\n",
    "    \n",
    "    Note that any token which is not in p_v_y should be mapped to\n",
    "    \"<unk>\". Further, the input dictionaries are probabilities. You\n",
    "    should convert them to log-probabilities while you compute\n",
    "    the Naive Bayes prediction rule to prevent underflow errors.\n",
    "    \n",
    "    Returns two values:\n",
    "        predictions: A list of integer labels, one for each document,\n",
    "            that is the predicted label for each instance.\n",
    "        confidences: A list of floats, one for each document, that is\n",
    "            p(y|d) for the corresponding label that is returned.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_semi_supervised(X_sup, y_sup, D_unsup, X_unsup, D_valid, y_valid, k, vocab, mode):\n",
    "    \"\"\"\n",
    "    Trains the Naive Bayes classifier using the semi-supervised algorithm.\n",
    "    \n",
    "    X_sup: A list of the featurized supervised documents.\n",
    "    y_sup: A list of the corresponding supervised labels.\n",
    "    D_unsup: The unsupervised documents.\n",
    "    X_unsup: The unsupervised document representations.\n",
    "    D_valid: The validation documents.\n",
    "    y_valid: The validation labels.\n",
    "    k: The smoothing parameter for Naive Bayes.\n",
    "    vocab: The vocabulary as a set of tokens.\n",
    "    mode: either \"threshold\" or \"top-k\", depending on which selection\n",
    "        algorithm should be used.\n",
    "    \n",
    "    Returns the final p_y and p_v_y (see `train_naive_bayes`) after the\n",
    "    algorithm terminates.    \n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that are named D_* are lists of documents where each\n",
    "# document is a list of tokens. y_* is a list of integer class labels.\n",
    "# X_* is a list of the feature dictionaries for each document.\n",
    "D_train, y_train = load_dataset('data/train.jsonl')\n",
    "D_valid, y_valid = load_dataset('data/valid.jsonl')\n",
    "D_test, y_test = load_dataset('data/test.jsonl')\n",
    "\n",
    "vocab = get_vocabulary(D_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the features, for example, using the BBowFeaturizer.\n",
    "# You actually only need to conver the training instances to their\n",
    "# feature-based representations.\n",
    "# \n",
    "# This is just starter code for the experiment. You need to fill in\n",
    "# the rest.\n",
    "featurizer = BBoWFeaturizer()\n",
    "X_train = convert_to_features(D_train, featurizer, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
