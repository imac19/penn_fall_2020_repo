{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVLSo4TR0aQU"
   },
   "source": [
    "# CIS 519 Homework 2: Linear Classifiers\n",
    "\n",
    "- Handed Out: October 5, 2020\n",
    "- Due: October 19, 2020 at 11:59pm.\n",
    "\n",
    "## Name: Ian MacDonald\n",
    "\n",
    "Although the solutions are my own, I consulted with the following people\n",
    "while working on this homework:\n",
    "- TODO (if applicable): ...\n",
    "\n",
    "## Preface\n",
    "\n",
    "- Feel free to talk to other members of the class in doing the homework. I am more concerned that you learn how to solve the problem than that you demonstrate that you solved it entirely on your own. You should, however, **write down your solution yourself**. Please include here the list of people you consulted with in the course of working on the homework:\n",
    "\n",
    "- While we encourage discussion within and outside the class, cheating and copying code is strictly not allowed. Copied code will result in the entire assignment being discarded at the very least.\n",
    "\n",
    "- Please use Piazza if you have questions about the homework. Also, please come to the TAs recitations and to the office hours.\n",
    "\n",
    "- The homework is due at 11:59 PM on the due date. We will be using Gradescope for collecting the homework assignments. You should have been automatically added to Gradescope. If not, please ask a TA for assistance. Post on Piazza and contact the TAs if you are having technical difficulties in submitting the assignment.\n",
    "\n",
    "- Here are some resources you will need for this assignment (https://www.seas.upenn.edu/~cis519/fall2020/assets/HW/HW2/hw2-materials.zip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClICZP2npLtE"
   },
   "source": [
    "# Overview\n",
    "\n",
    "### About Jupyter Notebooks\n",
    "\n",
    "In this homework assignment, we will use a Jupyter notebook to implement, analyze, and discuss ML classifiers.\n",
    "Knowing and being comfortable with Jupyter notebooks is a must in every data scientist, ML engineer, researcher, etc. They are widely used in industry and are a standard form of communication in ML by intertwining text and code to \"tell a story\". There are many resources that can introduce you to Jupyter notebooks (they are pretty easy to understand!), and if you still need help any of the TAs are more than willing to help.\n",
    "\n",
    "We will be using a local instance of Jupyter instead of Colab. You are of course free to use Colab, but you will need to understand how to hook your Colab instance with Google Drive to upload the datasets and to save images.\n",
    "\n",
    "\n",
    "\n",
    "### About the Homework\n",
    "\n",
    "You will experiment with several different linear classifiers and analyze \n",
    "their performances in both real and synthetic datasets. The goal is to understand the differences and\n",
    "similarities between the algorithms and the impact that the dataset characteristics have on the\n",
    "algorithms' learning behaviors and performances.\n",
    "\n",
    "In total, there are seven different learning algorithms which you will implement.\n",
    "Six are variants of the Perceptron algorithm and the seventh is a support vector machine (SVM).\n",
    "The details of these models is described in Section 1.\n",
    "\n",
    "\n",
    "In order to evaluate the performances of these models, you will use several different datasets.\n",
    "The first two datasets are synthetic datasets that have features and labels that were programatically\n",
    "generated. They were generated using the same script but use different input parameters that produced \n",
    "sparse and dense variants. The second two datasets are for the task of named-entity recognition (NER),\n",
    "identifying the names of people, locations, and organizations within text.\n",
    "One comes from news text and the other from a corpus of emails.\n",
    "For these two datasets, you need to implement the feature extraction yourself.\n",
    "All of the datasets and feature extraction information are described in Section 2.\n",
    "\n",
    "Finally, you will run two sets of experiments, one on the synthetic data and one on the NER data.\n",
    "The first set will analyze how the amount of training data impacts model performance.\n",
    "The second will look at the consequences of having training and testing data that come from different domains.\n",
    "The details of the experiments are described in Section 3.\n",
    "\n",
    "### Distribution of Points\n",
    "\n",
    "The homework has 4 sections for a total of 100 points + 10 extra credit points:\n",
    "- Section 0: Warmup (5 points)\n",
    "- Section 1: Linear Classifiers (30 points)\n",
    "- Section 2: Datasets (0 points, just text)\n",
    "- Section 3: Experiments (65 points)\n",
    "    - Synthetic Experiment:\n",
    "        - Parameter Tuning (10 points)\n",
    "        - Learning Curves(10 points)\n",
    "        - Final Test Accuracies (5 points)\n",
    "        - Discussion Questions (5 points)\n",
    "        - Noise Experiment (10 points **extra credit**)\n",
    "    - NER Experiment:\n",
    "        - Feature Extraction (25 points)\n",
    "        - Final Test Accuracies (5 points)\n",
    "        - $F_1$ Discussion Questions (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4gI2-Ygpr69"
   },
   "source": [
    "# Section 0: Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecz8xJo-ojUs"
   },
   "source": [
    "###### Only For Colab\n",
    "\n",
    "If you want to complete this homework in Colab, you are more than welcome to.\n",
    "You will need a little bit more maneuvering since you will need to upload\n",
    "the files of hw2 to your Google Drive and run the following two cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 16579,
     "status": "ok",
     "timestamp": 1601838226119,
     "user": {
      "displayName": "Samuel Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKvzcGAASWuvxjrEp7uMipUJWbOgy2JiGGaHeOtg=s64",
      "userId": "11877295110173304666"
     },
     "user_tz": 240
    },
    "id": "PUMTPLGG-3s2",
    "outputId": "9f08f1c8-19ed-4983-e733-5586d32d5649"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you want to use Colab for this homework.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1601838234837,
     "user": {
      "displayName": "Samuel Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKvzcGAASWuvxjrEp7uMipUJWbOgy2JiGGaHeOtg=s64",
      "userId": "11877295110173304666"
     },
     "user_tz": 240
    },
    "id": "eL4xKfSbop6z",
    "outputId": "d7f3736e-791d-466e-d836-c71d44d2e87d"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you want to use Colab for this homework.\n",
    "# %cd /content/drive/My Drive/Colab Notebooks/YOUR_PATH_TO_HW_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwXv78-V_wL_"
   },
   "source": [
    "###### Python Version\n",
    "\n",
    "Python 3.6 or above is required for this homework. Make sure you have it installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T5ANvqaM_wMA"
   },
   "outputs": [],
   "source": [
    "# Let's check.\n",
    "import sys\n",
    "if sys.version_info[:2] < (3, 6):\n",
    "    raise Exception(\"You have Python version \" + str(sys.version_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0fVHRxp0aQX"
   },
   "source": [
    "## Imports and Helper Functions (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2feRgjW0aQY"
   },
   "source": [
    "Let's import useful modules we will need throughout the homework\n",
    "as well as implement helper functions for our experiment. **Read and remember** what each function is doing, as you will probably need some of them down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ah78siKM0aQZ"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries for this homework.\n",
    "# %pip install sklearn\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "s4IV1W4y0aQg"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "DATASETS_PATH = \"datasets/\"\n",
    "NER_PATH = os.path.join(DATASETS_PATH, 'ner')\n",
    "SYNTHETIC_PATH = os.path.join(DATASETS_PATH, 'synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UdcTYi0w0aQl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that loads a synthetic dataset from the dataset root (e.g. \"synthetic/sparse\").\n",
    "\n",
    "You should not need to edit this method.\n",
    "\"\"\"\n",
    "def load_synthetic_data(dataset_type):\n",
    "\n",
    "    def load_jsonl(file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "        return data\n",
    "\n",
    "    def load_txt(file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(int(line.strip()))\n",
    "        return data\n",
    "\n",
    "    def convert_to_sparse(X):\n",
    "        sparse = []\n",
    "        for x in X:\n",
    "            data = {}\n",
    "            for i, value in enumerate(x):\n",
    "                if value != 0:\n",
    "                    data[str(i)] = value\n",
    "            sparse.append(data)\n",
    "        return sparse\n",
    "\n",
    "    path = os.path.join(SYNTHETIC_PATH, dataset_type)\n",
    "    \n",
    "    X_train = load_jsonl(os.path.join(path, 'train.X'))\n",
    "    X_dev = load_jsonl(os.path.join(path, 'dev.X'))\n",
    "    X_test = load_jsonl(os.path.join(path, 'test.X'))\n",
    "\n",
    "    num_features = len(X_train[0])\n",
    "    features = [str(i) for i in range(num_features)]\n",
    "\n",
    "    X_train = convert_to_sparse(X_train)\n",
    "    X_dev = convert_to_sparse(X_dev)\n",
    "    X_test = convert_to_sparse(X_test)\n",
    "\n",
    "    y_train = load_txt(os.path.join(path, 'train.y'))\n",
    "    y_dev = load_txt(os.path.join(path, 'dev.y'))\n",
    "    y_test = load_txt(os.path.join(path, 'test.y'))\n",
    "\n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rYO0o0VB0aQp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that loads the NER data from a path (e.g. \"ner/conll/train\"). \n",
    "\n",
    "You should not need to edit this method.\n",
    "\"\"\"\n",
    "def load_ner_data(dataset=None, dataset_type=None):\n",
    "    # List of tuples for each sentence\n",
    "    data = []\n",
    "    path = os.path.join(os.path.join(NER_PATH, dataset), dataset_type)\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r') as file:\n",
    "            sentence = []\n",
    "            for line in file:\n",
    "                if line == '\\n':\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "                else:\n",
    "                    sentence.append(tuple(line.split()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YYpjru5b0aQu"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A helper function that plots the relationship between number of examples\n",
    "and accuracies for all the models.\n",
    "\n",
    "You should not need to edit this method.\n",
    "\"\"\"\n",
    "def plot_learning_curves(\n",
    "    perceptron_accs,\n",
    "    winnow_accs,\n",
    "    adagrad_accs,\n",
    "    avg_perceptron_accs,\n",
    "    avg_winnow_accs,\n",
    "    avg_adagrad_accs,\n",
    "    svm_accs\n",
    "):\n",
    "    \"\"\"\n",
    "    This function will plot the learning curve for the 7 different models.\n",
    "    Pass the accuracies as lists of length 11 where each item corresponds\n",
    "    to a point on the learning curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = [\n",
    "        ('perceptron', perceptron_accs),\n",
    "        ('winnow', winnow_accs),\n",
    "        ('adagrad', adagrad_accs),\n",
    "        ('avg-perceptron', avg_perceptron_accs),\n",
    "        ('avg-winnow', avg_winnow_accs),\n",
    "        ('avg-adagrad', avg_adagrad_accs),\n",
    "        ('svm', svm_accs)\n",
    "    ]\n",
    "    \n",
    "    x = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 10000]\n",
    "    plt.figure()\n",
    "    f, (ax, ax2) = plt.subplots(1, 2, sharey=True, facecolor='w')\n",
    "    \n",
    "    for label, acc_list in accuracies:\n",
    "        assert len(acc_list) == 11\n",
    "        ax.plot(x, acc_list, label=label)\n",
    "        ax2.plot(x, acc_list, label=label)\n",
    "        \n",
    "    ax.set_xlim(0, 5500)\n",
    "    ax2.set_xlim(9500, 10000)\n",
    "    ax2.set_xticks([10000])\n",
    "    # hide the spines between ax and ax2\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.tick_params(labelright='off')\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sg9QfIak0aQz"
   },
   "source": [
    "### F1 Score (5 points)\n",
    "\n",
    "For some part of the homework, you will use the F1 score instead of accuracy to evaluate how well a model does. The F1 score is computed as the harmonic mean of the precision and recall of the classifier. Precision measures the number of correctly identified positive results by the total number of positive results. Recall, on the other hand, measures the number of correctly identified positive results divided by the number of all samples that should have been identified as positive. More formally, we have that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Precision} &= \\frac{TP}{TP + FP} \\\\\n",
    "\\text{Recall} &= \\frac{TP}{TP + FN}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $TP$ is the number of true positives, $FP$ false positives and $FN$ false negatives. Combining these two we define F1 as\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "You now need to implement the calculation of F1 yourself using the provided function header. It will be unit tested on Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WnzC-Gce0aQ0"
   },
   "outputs": [],
   "source": [
    "def calculate_f1(y_gold, y_model):\n",
    "    \n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "    fn_count = 0 \n",
    "    \n",
    "    for i in range (0, len(y_gold)):\n",
    "        \n",
    "        if(y_model[i] == 1):\n",
    "            \n",
    "            if(y_gold[i] == 1):\n",
    "                \n",
    "                tp_count+=1\n",
    "            \n",
    "            if(y_gold[i] != 1):\n",
    "                \n",
    "                fp_count+=1\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if(y_gold[i] == 1):\n",
    "                \n",
    "                fn_count+=1\n",
    "                \n",
    "    precision = tp_count/(tp_count+fp_count)\n",
    "        \n",
    "    recall = tp_count/(tp_count+fn_count)\n",
    "        \n",
    "    f1 = 2*(precision*recall)/(precision+recall)\n",
    "        \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pbt52HH0aQ7"
   },
   "source": [
    "Looking at the formula for the F1 score, what is the highest and lowest possible value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jB4UrDGR0aQ8"
   },
   "outputs": [],
   "source": [
    "def highest_and_lowest_f1_score():\n",
    "    return 1, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS0BO0AJ0aRA"
   },
   "source": [
    "# Section 1. Linear Classifiers (30 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM2eSe030aRB"
   },
   "source": [
    "This section details the seven different algorithms that you will use in the experiments. For each of the algorithms, we describe the initialization you should use to start training and the different parameter settings that you should use for the experiment on the synthetic datasets. Each of the update functions for the Perceptron, Winnow, and Perceptron with AdaGrad will be unittested on Gradescope, so please do not edit the function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YahdyT4H0aRC"
   },
   "source": [
    "### 1.1 Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kRcseeXa0aRD"
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"\n",
    "    DO NOT MODIFY\n",
    "\n",
    "    The Classifier class is the base class for all of the Perceptron-based\n",
    "    algorithms. Your class should override the \"process_example\" and\n",
    "    \"predict_single\" functions. Further, the averaged models should\n",
    "    override the \"finalize\" method, where the final parameter values\n",
    "    should be calculated. \n",
    "    \n",
    "    You should not need to edit this class any further.\n",
    "    \"\"\"\n",
    "    \n",
    "    ITERATIONS = 10\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        for iteration in range(self.ITERATIONS):\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                self.process_example(x_i, y_i)\n",
    "        self.finalize()\n",
    "\n",
    "    def process_example(self, x, y):\n",
    "        \"\"\"\n",
    "        Makes a prediction using the current parameter values for\n",
    "        the features x and potentially updates the parameters based\n",
    "        on the gradient. Note \"x\" is a dictionary which maps from the feature\n",
    "        name to the feature value and y is either 1 or -1.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Calculates the final parameter values for the averaged models.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts labels for all of the input examples. You should not need\n",
    "        to override this method.\n",
    "        \"\"\"\n",
    "        return [self.predict_single(x) for x in X]\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predicts a label, 1 or -1, for the input example. \"x\" is a dictionary\n",
    "        which maps from the feature name to the feature value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzRKTNaK0aRH"
   },
   "source": [
    "### 1.2 Basic Perceptron (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVvMIuX10aRI"
   },
   "source": [
    "We do this classifier for you, so enjoy the two free points and pay attention to the techniques and code written.\n",
    "\n",
    "#### 1.2.1 Description\n",
    "\n",
    "This is the basic version of the Perceptron Algorithm.\n",
    "    In this version, an update will be performed on the example $(\\textbf{x}, y)$ if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) \\leq 0$. The Perceptron needs to learn both the bias term $\\theta$ and the weight vector $\\mathbf{w}$ parameters.\n",
    "When the Perceptron makes a mistake on the example $(\\textbf{x}, y)$, both $\\mathbf{w}$ and $\\theta$ need to be updated using the following update equations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{w}^\\textrm{new} &\\gets \\mathbf{w} + \\eta \\cdot y \\cdot \\mathbf{x} \\\\\n",
    "    \\theta^\\textrm{new} &\\gets \\theta + \\eta \\cdot y\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "#### 1.2.2 Hyperparameters\n",
    "\n",
    "We set $\\eta$ to 1, so there are no hyperparameters to tune. \n",
    "\n",
    "Note: If we assume that the order of the examples presented to the algorithm is fixed, we initialize $\\mathbf{w} = \\mathbf{0}$ and $\\theta = 0$, and train both together, then the learning rate $\\eta$ does not have any effect.\n",
    "        In fact you can show that, if $\\mathbf{w}_1$ and $\\theta_1$ are the outputs of the Perceptron algorithm with learning rate $\\eta_1$, then $\\mathbf{w}_1/\\eta_1$ and $\\theta_1/\\eta_1$ will be the result of the Perceptron with learning rate 1 (note that these two hyperplanes give identical predictions).\n",
    "\n",
    "#### 1.2.3 Initialization\n",
    "\n",
    "$\\mathbf{w} = [0, 0, \\dots, 0]$ and $\\theta = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zotWbBvj0aRJ"
   },
   "outputs": [],
   "source": [
    "class Perceptron(Classifier):\n",
    "    \"\"\"\n",
    "    DO NOT MODIFY THIS CELL\n",
    "\n",
    "    The Perceptron model. Note how we are subclassing `Classifier`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Initializes the parameters for the Perceptron model. \"features\"\n",
    "        is a list of all of the features of the model where each is\n",
    "        represented by a string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # NOTE: Do not change the names of these 3 data members because\n",
    "        # they are used in the unit tests\n",
    "        self.eta = 1\n",
    "        self.theta = 0\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "\n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        if y != y_pred:\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] += self.eta * y * value\n",
    "            self.theta += self.eta * y\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaiyRKjo0aRN"
   },
   "source": [
    "For the rest of the Perceptron-based algorithms, you will have to implement the corresponding class like we have done for `Perceptron`.\n",
    "Use the `Perceptron` class as a guide for how to implement the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWiuhyOm0aRO"
   },
   "source": [
    "### 1.3 Winnow (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9dx93VU0aRR"
   },
   "source": [
    "#### 1.3.1 Description\n",
    "The Winnow algorithm is a variant of the Perceptron algorithm with multiplicative updates. Since the algorithm requires that the target function is monotonic, you will only use it on the synthetic datasets.\n",
    "\n",
    "The Winnow algorithm only learns parameters $\\mathbf{w}$.\n",
    "We will fix $\\theta = -n$, where $n$ is the number of features.\n",
    "When the Winnow algorithm makes a mistake on the example $(\\textbf{x}, y)$, the parameters are updated with the following equation:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    w^\\textrm{new}_i \\gets w_i \\cdot \\alpha^{y \\cdot x_i}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $w_i$ and $x_i$ are the $i$th components of the corresponding vectors.\n",
    "Here, $\\alpha$ is a promotion/demotion hyperparameter.\n",
    "\n",
    "#### 1.3.2 Hyperparameters\n",
    "\n",
    "For the experiment, choose $\\alpha \\in \\{1.1, 1.01, 1.005, 1.0005, 1.0001\\}$.\n",
    "\n",
    "#### 1.3.3 Initialization\n",
    "\n",
    "$\\mathbf{w} = [1, 1, \\dots, 1]$ and $\\theta = -n$ (constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g9pPUPTd0aRS"
   },
   "outputs": [],
   "source": [
    "class Winnow(Classifier):\n",
    "    def __init__(self, alpha, features):\n",
    "        # DO NOT change the names of these 3 data members because\n",
    "        # they are used in the unit tests\n",
    "        self.alpha = alpha\n",
    "        self.w = {feature: 1.0 for feature in features}\n",
    "        self.theta = -len(features)\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        if (y_pred != y):\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] = self.w[feature] * self.alpha**(y*value)\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if (score <= 0):\n",
    "            return -1 \n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--I-w3_U0aRX"
   },
   "source": [
    "### 1.4 AdaGrad (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5AXEuKn0aRY"
   },
   "source": [
    "#### 1.4.1 Description\n",
    "AdaGrad is a variant of the Perceptron algorithm that adapts the learning rate for each parameter based on historical information.\n",
    "    The idea is that frequently changing features get smaller learning rates and stable features higher ones.\n",
    "\n",
    "To derive the update equations for this model, we first need to start with the loss function.\n",
    "Instead of using the hinge loss with the elbow at 0 (like the basic Perceptron does), we will instead use the standard hinge loss with the elbow at 1:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\mathbf{x}, y, \\mathbf{w}, \\theta) = \\max\\left\\{0, 1 - y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta)\\right\\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Then, by taking the partial derivative of $\\mathcal{L}$ with respect to $\\mathbf{w}$ and $\\theta$, we can derive the respective graidents (make sure you understand how you could derive these gradients on your own):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{w}} &=\n",
    "        \\begin{cases}\n",
    "        \\mathbf{0} & \\text{if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) > 1$} \\\\\n",
    "        -y\\cdot \\mathbf{x} & \\textrm{otherwise}\n",
    "        \\end{cases} \\\\\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial\\theta} &=\n",
    "        \\begin{cases}\n",
    "            0 & \\text{if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) > 1$} \\\\\n",
    "            -y & \\textrm{otherwise}\n",
    "        \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then for each parameter, we will keep track of the sum of the parameters' squared gradients.\n",
    "In the following equations, the $k$ superscript refers to the $k$th non-zero gradient (i.e., the $k$th weight vector/misclassified example) and $t$ is the number of mistakes seen thus far.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    G^t_j &= \\sum_{k=1}^t \\left(\\frac{\\partial \\mathcal{L}}{\\partial w^k_j}\\right)^2 \\\\\n",
    "    H^t &= \\sum_{k=1}^t \\left(\\frac{\\partial \\mathcal{L}}{\\partial \\theta^k}\\right)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For example, on the 3rd mistake ($t = 3$), $G^3_j$ is the sum of the squares of the first three non-zero gradients ($\\left(\\frac{\\partial \\mathcal{L}}{\\partial w^1_j}\\right)^2$, $\\left(\\frac{\\partial \\mathcal{L}}{\\partial w^2_j}\\right)^2$, and $\\left(\\frac{\\partial \\mathcal{L}}{\\partial w^3_j}\\right)^2$).\n",
    "Then $\\mathbf{G}^3$ is used to calculate the 4th value of the weight vector as follows.\n",
    "On example $(\\mathbf{x}, y)$, if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) \\leq 1$, then the parameters are updated with the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{w}^{t+1} &\\gets \\mathbf{w}^t + \\eta \\cdot \\frac{y \\cdot \\mathbf{x}}{\\sqrt{\\mathbf{G}^t}} \\\\\n",
    "    \\theta^{t+1} &\\gets \\theta^t + \\eta \\frac{y}{\\sqrt{H^t}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that, although we use the hinge loss with the elbow at 1 for training, you still make the prediction based on whether or not $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) \\leq 0$ during testing.\n",
    "\n",
    "#### 1.4.2 Hyperparameters\n",
    "\n",
    "For the experiment, choose $\\eta \\in \\{1.5, 0.25, 0.03, 0.005, 0.001\\}$\n",
    "\n",
    "#### 1.4.3 Initialization\n",
    "\n",
    "$\\mathbf{w} = [0, 0, \\dots, 0]$ and $\\theta = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "w50RXTza0aRZ"
   },
   "outputs": [],
   "source": [
    "class AdaGrad(Classifier):\n",
    "    def __init__(self, eta, features):\n",
    "        # DO NOT change the names of these 3 data members because\n",
    "        # they are used in the unit tests\n",
    "        self.eta = eta\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "        self.theta = 0\n",
    "        self.G = {feature: 1e-5 for feature in features}  # 1e-5 prevents divide by 0 problems\n",
    "        self.H = 0\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        if (y_pred != y):\n",
    "            for feature, value in x.items():\n",
    "                dw = y*value\n",
    "                self.G[feature]+=(dw**2)\n",
    "                self.w[feature] = self.w[feature] + (self.eta*y*value)/(np.sqrt(self.G[feature]))\n",
    "                \n",
    "            dt = -y\n",
    "            self.H += dt**2\n",
    "            self.theta = self.theta + (self.eta*y)/(np.sqrt(self.H))\n",
    "                \n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if (score <= 0):\n",
    "            return -1 \n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmAio9O40aRd"
   },
   "source": [
    "### 1.5 Averaged Models (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzPnbRJ60aRe"
   },
   "source": [
    "You will also implement the averaged version of the previous three algorithms.\n",
    "\n",
    "During the course of training, each of the above algorithms will have $K + 1$ different parameter settings for the $K$ different updates it will make during training.\n",
    "The regular implementation of these algorithms uses the parameter values after the $K$th update as the final ones.\n",
    "Instead, the averaged version use the weighted average of the $K + 1$ parameter values as the final parameter values.\n",
    "Let $m_k$ denote the number of correctly classified examples by the $k$th parameter values and $M$ the total number of correctly classified examples.\n",
    "The final parameter values are\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    M &= \\sum_{k=1}^{K+1} m_k \\\\\n",
    "    \\mathbf{w} &\\gets \\frac{1}{M} \\sum_{k=1}^{K+1} m_k \\cdot \\mathbf{w}^k \\\\\n",
    "    \\theta &\\gets \\frac{1}{M} \\sum_{k=1}^{K+1} m_k \\cdot \\theta^k \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For each of the averaged versions of Perceptron, Winnow, and AdaGrad, use the same hyperparameters and initialization as before.\n",
    "\n",
    "#### 1.5.1 Implementation Note\n",
    "Implementing the averaged variants of these algorithms can be tricky.\n",
    "    While the final parameter values are based on the sum of $K$ different vectors, there is no need to maintain *all* of these parameters.\n",
    "    Instead, you should implement these algorithms by keeping only two vectors, one which maintains the cumulative sum and the current one.\n",
    "\n",
    "Additionally, there are two ways of keeping track of these two vectors.\n",
    "One is more straightforward but prohibitively slow.\n",
    "The second requires some algebra to derive but is significantly faster to run.\n",
    "Try to analyze how the final weight vector is a function of the intermediate updates and their corresponding weights.\n",
    "It should take less than a minute or two for ten iterations for any of the averaged algorithms.\n",
    "**You need to think about how to efficiently implement the averaged algorithms yourself.**\n",
    "\n",
    "Further, the implementation for Winnow is slightly more complicated than the other two, so if you consistently have low accuracy for the averaged Winnow, take a closer look at the derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Vo-NP3ot0aRf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AveragedPerceptron(Classifier):\n",
    "    def __init__(self, features):\n",
    "        self.eta = 1\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "        self.theta = 0\n",
    "        self.cumul_w = {feature: 0.0 for feature in features}\n",
    "        self.cumul_theta = 0\n",
    "        self.classified = 0\n",
    "        self.final_m = 0\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        self.final_m+=1\n",
    "        y_pred = self.predict_single(x)\n",
    "        \n",
    "        if (y_pred == y):\n",
    "            self.classified+=1\n",
    "        \n",
    "        else:\n",
    "            for feature in self.w:\n",
    "                self.cumul_w[feature] += self.w[feature] * self.classified\n",
    "                if feature in x:\n",
    "                    self.w[feature] += self.eta * y * x[feature]\n",
    "            self.cumul_theta += self.theta * self.classified\n",
    "            self.theta += self.eta * y\n",
    "            self.classified=1\n",
    "            \n",
    "#         y_pred = self.predict_single(x)\n",
    "#         self.classified+=1\n",
    "#         if y != y_pred:\n",
    "#             for feature in self.w:\n",
    "#                 if feature in x:\n",
    "#                     self.w[feature] += self.eta * y * x[feature]\n",
    "            \n",
    "#                 self.cumul_w[feature] += self.w[feature]\n",
    "            \n",
    "#             self.theta += self.eta * y\n",
    "#             self.cumul_theta += self.theta\n",
    "        \n",
    "#         else:\n",
    "#             for feature in self.w:\n",
    "#                 self.cumul_w[feature] += self.w[feature]\n",
    "            \n",
    "#             self.cumul_theta += self.theta\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1\n",
    "        \n",
    "    def finalize(self):\n",
    "        for feature in self.w:\n",
    "            self.cumul_w[feature] += self.w[feature] * self.classified\n",
    "        self.cumul_theta += self.theta * self.classified\n",
    "        \n",
    "        final_w = {feature: (value/self.final_m) for feature, value in self.cumul_w.items()}\n",
    "        final_theta = self.cumul_theta/self.final_m\n",
    "        \n",
    "        self.w = final_w\n",
    "        self.theta = final_theta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bv-kvuF30aRj"
   },
   "outputs": [],
   "source": [
    "class AveragedWinnow(Classifier):\n",
    "    def __init__(self, alpha, features):\n",
    "        self.alpha = alpha\n",
    "        self.w = {feature: 1.0 for feature in features}\n",
    "        self.theta = -len(features)\n",
    "        self.cumul_w = {feature: 1.0 for feature in features}\n",
    "        self.classified = 0\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        self.classified+=1\n",
    "        if y != y_pred:\n",
    "            for feature in self.w:\n",
    "                if feature in x:\n",
    "                    self.w[feature] = self.w[feature] * self.alpha**(y*x[feature])\n",
    "            \n",
    "                self.cumul_w[feature] += self.w[feature]\n",
    "        \n",
    "        else:\n",
    "            for feature in self.w:\n",
    "                self.cumul_w[feature] += self.w[feature]\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if (score <= 0):\n",
    "            return -1 \n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def finalize(self):\n",
    "        final_m = self.classified\n",
    "        final_w = {feature: (value/final_m) for feature, value in self.cumul_w.items()}\n",
    "        \n",
    "        self.w = final_w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "L9xWrGHS0aRn"
   },
   "outputs": [],
   "source": [
    "class AveragedAdaGrad(Classifier):\n",
    "    def __init__(self, eta, features):\n",
    "        self.eta = eta\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "        self.theta = 0\n",
    "        self.G = {feature: 1e-5 for feature in features}\n",
    "        self.H = 0\n",
    "        self.cumul_w = {feature: 0.0 for feature in features}\n",
    "        self.cumul_theta = 0\n",
    "        self.classified = 0 \n",
    "        self.final_m = 0 \n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        self.final_m+=1\n",
    "        \n",
    "        if (y_pred == y):\n",
    "            self.classified+=1\n",
    "            \n",
    "        else:\n",
    "            for feature in self.w:\n",
    "                self.cumul_w[feature] += self.w[feature] * self.classified\n",
    "                if feature in x:\n",
    "                    dw=y*x[feature]\n",
    "                    self.G[feature]+=(dw**2)\n",
    "                    self.w[feature]=self.w[feature] + (self.eta*y*x[feature])/(np.sqrt(self.G[feature]))\n",
    "\n",
    "            dt = -y\n",
    "            self.H += dt**2\n",
    "            self.cumul_theta += self.theta * self.classified\n",
    "            self.theta = self.theta + (self.eta*y)/(np.sqrt(self.H))\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if (score <= 0):\n",
    "            return -1 \n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def finalize(self):\n",
    "        for feature in self.w:\n",
    "            self.cumul_w[feature] += self.w[feature] * self.classified\n",
    "        self.cumul_theta += self.theta * self.classified\n",
    "        \n",
    "        final_w = {feature: (value/self.final_m) for feature, value in self.cumul_w.items()}\n",
    "        final_theta = self.cumul_theta/self.final_m\n",
    "        \n",
    "        self.w = final_w\n",
    "        self.theta = final_theta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMi5gf710aRr"
   },
   "source": [
    "### 1.6 Support Vector Machines\n",
    "\n",
    "Although we have not yet covered SVMs in class, you can still train them using the `sklearn` library.\n",
    "We will use a soft margin SVM for non-linearly separable data.\n",
    "You should use the `sklearn` implementation as follows:\n",
    "```\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC(loss='hinge')\n",
    "classifier.fit(X, y)\n",
    "```\n",
    "\n",
    "`sklearn` requires a different feature representation than what we use for the Perceptron models.\n",
    "The provided Python template code demonstrates how to convert to the require representation.\n",
    "\n",
    "\n",
    "Given training samples $S = \\{(\\mathbf{x}^1, y^1), (\\mathbf{x}^2, y^2), \\dots, (\\mathbf{x}^m, y^m)\\}$, the objective for the SVM is the following:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2} \\vert\\vert \\mathbf{w}\\vert\\vert^2_2 + C \\sum_{i=1}^m \\xi_i\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "subject to the following constraints:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y^i(\\mathbf{w}^\\intercal \\mathbf{x}^i + b) \\geq 1 - \\xi_i \\;\\;&\\textrm{for } i = 1, 2, \\dots, m \\\\\n",
    "    \\xi_i \\geq 0 \\;\\;& \\textrm{for } i = 1, 2, \\dots, m\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzmeTfF20aR0"
   },
   "source": [
    "# Section 2. Datasets\n",
    "\n",
    "In this section, we describe the synthetic and NER datasets that you will use for your experiments.\n",
    "For the NER datasets, there is also an explanation for the features which you need to extract from the data.\n",
    "\n",
    "### 2.1 Synthetic Data\n",
    "\n",
    "#### 2.1.1 Introduction\n",
    "\n",
    "The synthetic datasets have features and labels which are automatically generated from a python script.\n",
    "Each instance will have $n$ binary features and are labeled according to a $l$-of-$m$-of-$n$ Boolean function.\n",
    "Specifically, there is a set of $m$ features such that an example if positive if and only if at least $l$ of these $m$ features are active.\n",
    "The set of $m$ features is the same for the dataset (i.e., it is not a separate set of $m$ features for each individual instance).\n",
    "\n",
    "We provide two versions of the synthetic dataset called sparse and dense.\n",
    "For both datasets, we set $l = 10$ and $m=20$.\n",
    "We set $n = 200$ for the sparse data and $n = 40$ for the dense data.\n",
    "Additionally, we add noise to the data as follows:\n",
    "With probability $0.05$ the label assigned by the function is changed and with probability $0.001$ each feature value is changed.\n",
    "Consequently, the data is not linearly separable.\n",
    "\n",
    "We have provided you with three data splits for both sparse and dense with 10,000 training, 2,000 development, and 2,000 testing examples.\n",
    "Section 3 describes the experiments that you need to run on these datasets.\n",
    "\n",
    "#### 2.1.2 Feature Representation\n",
    "\n",
    "The features of the synthetic data provided are vectors of 0s and 1s.\n",
    "Storing these large matrices requires lots of memory so we use a sparse representation that stores them as dictionaries instead.\n",
    "For example, the vector $[0,1,0,0,0,1]$ can be stored as `{\"x2\": 1,\"x6\": 1}` (using 1-based indexing).\n",
    "We have provided you with the code for parsing and converting the data to this format.\n",
    "You can use these for the all algorithms you develop except the SVM.\n",
    "Since you will be using the implementation of SVM from sklearn, you will need to provide a vector to it. You can use `sklearn.feature_extraction.DictVectorizer` for converting feature-value dictionaries to vectors.\n",
    "\n",
    "### 2.2 NER Data\n",
    "\n",
    "In addition to the synthetic data, we have provided you two datasets for the task of named-entity recognition (NER).\n",
    "The goal is to identify whether strings in text represent names of people, organizations, or locations.\n",
    "An example instance looks like the following:\n",
    "\n",
    "```\n",
    "    [PER Wolff] , currently a journalist in [LOC Argentina] , played with\n",
    "    [PER del Bosque] in the final years of the seventies in\n",
    "    [ORG Real Madrid] .\n",
    "```\n",
    "\n",
    "In this problem, we will simplify the task to identifying whether a string is named entity or not (that is, you don't have to say which type of entity it is).\n",
    "For each token in the input, we will use the tag $\\texttt{I}$ to denote that token is an entity and $\\texttt{O}$ otherwise.\n",
    "For example, the full tagging for the above instace is as follows:\n",
    "\n",
    "```\n",
    "    [I Wolff] [O ,] [O currently] [a] [O journalist] [O in] [I Argentina]\n",
    "    [O ,] [O played] [O with] [I del] [I Bosque] [O in] [O the] [O final]\n",
    "    [O years] [O of] [O the] [O seventies] [O in] [I Real] [I Madrid] .\n",
    "```\n",
    "\n",
    "Given a sentence $S = w_1, w_2, \\dots, w_n$, you need to predict the `I`, `O` tag for each word in the sentence.\n",
    "That is, you will produce the sequence $Y = y_1, y_2, \\dots, y_n$ where $y_i \\in$ {`I`, `O`}.\n",
    "\n",
    "#### 2.2.1 Datasets: CoNLL and Enron\n",
    "\n",
    "We have provided two datasets, the CoNLL dataset which is text from news articles, and Enron, a corpus of emails.\n",
    "The files contain one word and one tag per line.\n",
    "For CoNLL, there are training, development, and testing files, whereas Enron only has a test dataset.\n",
    "There are 14,987 training sentences (204,567 words), 336 development sentences (3,779 words), and 303 testing sentences (3,880 words) in CoNLL.\n",
    "For Enron there are 368 sentences (11,852 words).\n",
    "\n",
    "**Please note that the CoNLL dataset is available only for the purposes of this assignment.\n",
    "It is copyrighted, and you are granted access because you are a Penn student, but please delete it when you are done with the homework.**\n",
    "\n",
    "#### 2.2.2 Feature Extraction\n",
    "\n",
    "The NER data is provided as raw text, and you are required to extract features for the classifier.\n",
    "In this assignment, we will only consider binary features based on the context of the word that is supposed to be tagged.\n",
    "\n",
    "Assume that there are $V$ unique words in the dataset and each word has been assigned a unique ID which is a number $\\{1, 2, \\dots, V\\}$.\n",
    "Further, $w_{-k}$ and $w_{+k}$ indicate the $k$th word before and after the target word.\n",
    "The feature templates that you should use to generate features are as follows:\n",
    "\n",
    "| Template             | Number of Features |\n",
    "|----------------------|--------------------|\n",
    "| $w_{-3}$             | $V$                |\n",
    "| $w_{-2}$             | $V$                |\n",
    "| $w_{-1}$             | $V$                |\n",
    "| $w_{+1}$             | $V$                |\n",
    "| $w_{+2}$             | $V$                |\n",
    "| $w_{+3}$             | $V$                |\n",
    "| $w_{-1}$ & $w_{-2}$  | $V \\times V$       |\n",
    "| $w_{+1}$ \\& $w_{+2}$ | $V \\times V$       |\n",
    "| $w_{-1}$ \\& $w_{+1}$ | $V \\times V$       |\n",
    "\n",
    "Each feature template corresponds to a set of features that you will compute (similar to the features you generated in problem 2 from the first homework assignment).\n",
    "The $w_{-3}$ feature template corresponds to $V$ features where the $i$th feature is 1 if the third word to the left of the target word has ID $i$.\n",
    "The $w_{-1} \\& w_{+1}$ feature template corresponds to $V \\times V$ features where there is one feature for every unique pair words.\n",
    "For example, feature $(i - 1) \\times V + j$ is a binary feature that is 1 if the word 1 to the left of the target has ID $i$ and the first word to the right of the target has ID $j$.\n",
    "In practice, you will not need to keep track of the feature IDs.\n",
    "Instead, each feature will be given a name such as \"$w_{-1}=\\textrm{the} \\& w_{+1}=\\textrm{cat}$\".\n",
    "\n",
    "In total, all of the above feature templates correspond to a very large number of features.\n",
    "However, for each word, there will be exactly 9 features which are active (non-zero), so the feature vector is quite sparse.\n",
    "You will represent this as a dictionary which maps from the feature name to the value.\n",
    "In the provided Python template, we have implemented a couple of the features for you to demonstrate how to compute them and what the naming scheme should look like.\n",
    "\n",
    "In order to deal with the first two words and the last two words in a sentence, we will add special symbol \"SSS\" and \"EEE\" to the vocabulary to represent the words before the first word and the words after the last word.\n",
    "Notice that in the test data you may encounter a word that was not observed in training, and therefore is not in your dictionary.\n",
    "In this case, you cannot generate a feature for it, resulting in less than 7 active features in some of the test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHHy_nGx0aR2"
   },
   "source": [
    "# Section 3. Experiments (65 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PKCZQ9Z0aR3"
   },
   "source": [
    "You will run two sets of experiments, one using the synthetic data and one using the NER data.\n",
    "\n",
    "### 3.1 Synthetic Experiment (30 + 10 extra credit points)\n",
    "\n",
    "This experiment will explore the impact that the amount of training data has on model performance.\n",
    "First, you will do hyperparameter tuning for Winnow and Perceptron with AdaGrad (both standard and averaged versions).\n",
    "Then you will generate learning curves that will plot the size of the training data against the performance.\n",
    "Finally, for each of the models trained on all of the training data, you will find the test score.\n",
    "You should use accuracy to compute the performance of the model.\n",
    "\n",
    "In summary, the experiment consists of three parts\n",
    "1. Parameter Tuning\n",
    "2. Learning Curves\n",
    "3. Final Evaluation\n",
    "\n",
    "#### 3.1.1 Parameter Tuning (10 points)\n",
    "\n",
    "For both the Winnow and Perceptron with AdaGrad (standard and averaged), there are hyperparameters that you need to choose.\n",
    "(The same is true for SVM, but you should only use the default settings.)\n",
    "Similarly to cross-validation from Homework 1, we will estimate how well each model will do on the true test data using the development dataset (we will not run cross-validation), and choose the hyperparameter settings based on these results.\n",
    "\n",
    "For each hyperparameter value in Section 1, train a model using that value on the training data and compute the accuracy on the development dataset. Each model should be trained for 10 iterations (i.e., 10 passes over the entire dataset).\n",
    "\n",
    "TODO: Fill in the table with the best hyperparameter values and the corresponding validation accuracies.\n",
    "Repeat this for both the sparse and dense data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data sets \n",
    "\n",
    "sparse_data = load_synthetic_data('sparse')\n",
    "dense_data = load_synthetic_data('dense')\n",
    "\n",
    "sparse_x_train = sparse_data[0]\n",
    "sparse_y_train = sparse_data[1]\n",
    "sparse_x_dev = sparse_data[2]\n",
    "sparse_y_dev = sparse_data[3]\n",
    "sparse_x_test = sparse_data[4]\n",
    "sparse_y_test = sparse_data[5]\n",
    "sparse_features = sparse_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(sparse_features)\n",
    "perceptron.train(sparse_x_train, sparse_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7737809752198241"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_model_dev_predictions = perceptron.predict(sparse_x_dev)\n",
    "calculate_f1(sparse_y_dev, perceptron_model_dev_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 318 µs, total: 2.24 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avg_perceptron = AveragedPerceptron(sparse_features)\n",
    "avg_perceptron.train(sparse_x_train, sparse_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9062034739454093"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_perceptron_model_dev_predictions = avg_perceptron.predict(sparse_x_dev)\n",
    "calculate_f1(sparse_y_dev, avg_perceptron_model_dev_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "winnow = Winnow(1.1, sparse_features)\n",
    "winnow.train(sparse_x_train, sparse_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8945022288261516"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winnow_model_dev_predictions = winnow.predict(sparse_x_dev)\n",
    "calculate_f1(sparse_y_dev, winnow_model_dev_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 3.99 ms, total: 3.06 s\n",
      "Wall time: 3.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avg_winnow = AveragedWinnow(1.1, sparse_features)\n",
    "avg_winnow.train(sparse_x_train, sparse_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9390609390609391"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_winnow_model_dev_predictions = avg_winnow.predict(sparse_x_dev)\n",
    "calculate_f1(sparse_y_dev, avg_winnow_model_dev_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaGrad(1.5, sparse_features)\n",
    "ada.train(sparse_x_train, sparse_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8651262761955937"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_model_dev_predictions = ada.predict(sparse_x_dev)\n",
    "calculate_f1(sparse_y_dev, ada_model_dev_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 2 µs, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "avg_ada = AveragedAdaGrad(1.1, sparse_features)\n",
    "avg_ada.train(sparse_x_train, sparse_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9265"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_ada_model_dev_predictions = avg_ada.predict(sparse_x_test)\n",
    "accuracy_score(sparse_y_test, avg_ada_model_dev_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(loss='hinge')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = DictVectorizer(sparse=False, dtype=int)\n",
    "svc_x_train=dv.fit_transform(sparse_x_train)\n",
    "svc.fit(svc_x_train, sparse_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.939"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_pred = svc.predict(dv.fit_transform(sparse_x_dev))\n",
    "accuracy_score(svc_pred, sparse_y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdoFHNKp0aR4"
   },
   "source": [
    "##### Winnow Sweep\n",
    "\n",
    "| $\\alpha$ | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.1      |        |       |\n",
    "| 1.01     |        |       |\n",
    "| 1.005    |        |       |\n",
    "| 1.0005   |        |       |\n",
    "| 1.0001   |        |       |\n",
    "\n",
    "##### Averaged Winnow Sweep\n",
    "\n",
    "| $\\alpha$ | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.1      |        |       |\n",
    "| 1.01     |        |       |\n",
    "| 1.005    |        |       |\n",
    "| 1.0005   |        |       |\n",
    "| 1.0001   |        |       |\n",
    "\n",
    "##### AdaGrad Sweep\n",
    "\n",
    "| $\\eta$   | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.5      |        |       |\n",
    "| 0.25     |        |       |\n",
    "| 0.03     |        |       |\n",
    "| 0.005    |        |       |\n",
    "| 0.001    |        |       |\n",
    "\n",
    "##### Averaged AdaGrad Sweep\n",
    "\n",
    "| $\\eta$   | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.5      |        |       |\n",
    "| 0.25     |        |       |\n",
    "| 0.03     |        |       |\n",
    "| 0.005    |        |       |\n",
    "| 0.001    |        |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHZZ6U240aR6"
   },
   "source": [
    "#### 3.1.2 Learning Curves (10 points)\n",
    "\n",
    "Next, you will train all 7 models with different amounts of training data.\n",
    "For Winnow and Perceptron with AdaGrad (standard and averaged), use the best hyperparameters from the parameter tuning experiment.\n",
    "\n",
    "Each of the datasets contains 10,000 training examples.\n",
    "You will train each model 11 times on varying amounts of training data.\n",
    "The first 10 will increase by 500 examples: 500, 1k, 1.5k, 2k, ..., 5k.\n",
    "The 11th model should use all 10k examples.\n",
    "Each Perceptron-based model should be trained for 10 iterations (e.g., 10 passes over the total number of training examples available to that model).\n",
    "The SVM can be run until convergence with the default parameters.\n",
    "\n",
    "For each model, compute the accuracy on the development dataset and plot the results using the provided code.\n",
    "There should be a separate plot for the sparse and dense data.\n",
    "\n",
    "**Note** how we have included an image in markdown. You should do the same for both plots and include them in the output below by running your experiment, saving your plots to the images folder, and linking it to this cell.\n",
    "\n",
    "##### Sparse Plot \n",
    "\n",
    "![Sample](images/sample.png)\n",
    "\n",
    "##### Dense Plot\n",
    "\n",
    "![Sample](images/sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wHcZsGo0aR8"
   },
   "source": [
    "#### 3.1.3 Final Evaluation (5 points)\n",
    "\n",
    "Finally, for each of the 7 models, train the models on all of the training data and compute the test accuracy.\n",
    "For Winnow and Perceptron with AdaGrad, use the best hyperparameter settings you found.\n",
    "Report these accuracies in a table.\n",
    "   \n",
    "We will run our models with [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 10000] examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NCfmjoYw_wNQ"
   },
   "outputs": [],
   "source": [
    "sample_sizes = [500 * i for i in range(1, 11)] + [10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "ZJJ9u_kf0aSB"
   },
   "outputs": [],
   "source": [
    "def run_synthetic_experiment(dataset_type):\n",
    "    \"\"\"\n",
    "    TODO: IMPLEMENT \n",
    "    \n",
    "    Runs the synthetic experiment on either the sparse or dense data\n",
    "    depending on the data path (e.g. \"data/sparse\" or \"data/dense\").\n",
    "    \n",
    "    We have provided how to train the Perceptron on the training and\n",
    "    test on the testing data (the last part of the experiment). You need\n",
    "    to implement the hyperparameter sweep, the learning curves, and\n",
    "    predicting on the test dataset for the other models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in data sets\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data(dataset_type)\n",
    "    \n",
    "    # Select model hyperparameters\n",
    "    \n",
    "    winnow_hp_choices = [1.1, 1.01, 1.005, 1.0005, 1.0001]\n",
    "    ada_hp_choices = [1.5, .25, .03, .005, .001]\n",
    "    \n",
    "    winnow_hp_accs = []\n",
    "    ada_hp_accs = []\n",
    "    winnow_avg_hp_accs = []\n",
    "    ada_avg_hp_accs = []\n",
    "    \n",
    "    for hp in winnow_hp_choices:\n",
    "        w = Winnow(hp, features)\n",
    "        w.train(X_train, y_train)\n",
    "        w_pred = w.predict(X_dev)\n",
    "        w_acc = accuracy_score(y_dev, w_pred)\n",
    "        winnow_hp_accs.append(w_acc)\n",
    "        print('Winnow Accuracy for Hyperparameter: ' + str(hp) + ' = ' + str(w_acc))\n",
    "        \n",
    "        w_avg = AveragedWinnow(hp, features)\n",
    "        w_avg.train(X_train, y_train)\n",
    "        w_avg_pred = w_avg.predict(X_dev)\n",
    "        w_avg_acc = accuracy_score(y_dev, w_avg_pred)\n",
    "        winnow_avg_hp_accs.append(w_avg_acc)\n",
    "        print('AveragedWinnow Accuracy for Hyperparameter: ' + str(hp) + ' = ' + str(w_avg_acc))\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    for hp in ada_hp_choices:\n",
    "        a = AdaGrad(hp, features)\n",
    "        a.train(X_train, y_train)\n",
    "        a_pred = a.predict(X_dev)\n",
    "        a_acc = accuracy_score(y_dev, a_pred)\n",
    "        ada_hp_accs.append(a_acc)\n",
    "        print('AdaGrad Accuracy for Hyperparameter: ' + str(hp) + ' = ' + str(a_acc))\n",
    "        \n",
    "        a_avg = AveragedAdaGrad(hp, features)\n",
    "        a_avg.train(X_train, y_train)\n",
    "        a_avg_pred = a_avg.predict(X_dev)\n",
    "        a_avg_acc = accuracy_score(y_dev, a_avg_pred)\n",
    "        ada_avg_hp_accs.append(a_avg_acc)\n",
    "        print('AveragedAdaGrad Accuracy for Hyperparameter: ' + str(hp) + ' = ' + str(a_avg_acc))\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    \n",
    "    winnow_hp_selection = winnow_hp_choices[winnow_hp_accs.index(max(winnow_hp_accs))]\n",
    "    ada_hp_selection = ada_hp_choices[ada_hp_accs.index(max(ada_hp_accs))]\n",
    "    winnow_avgd_hp_selection = winnow_hp_choices[winnow_avg_hp_accs.index(max(winnow_avg_hp_accs))]\n",
    "    ada_avgd_hp_selection = ada_hp_choices[ada_avg_hp_accs.index(max(ada_avg_hp_accs))]\n",
    "    \n",
    "    print('Ideal Winnow Hyperparameter: ' + str(winnow_hp_selection) + ', Accuracy: ' + str(max(winnow_hp_accs)))\n",
    "    print('Ideal AdaGrad Hyperparameter: ' + str(ada_hp_selection) + ', Accuracy: ' + str(max(ada_hp_accs)))\n",
    "    print('Ideal AveragedWinnow Hyperparameter: ' + str(winnow_avgd_hp_selection) + ', Accuracy: ' + str(max(winnow_avg_hp_accs)))\n",
    "    print('Ideal AveragedAdaGrad Hyperparameter: ' + str(ada_avgd_hp_selection) + ', Accuracy: ' + str(max(ada_avg_hp_accs)))\n",
    "    print()\n",
    "    \n",
    "#     winnow_hp_selection = winnow_hp_choices[0]\n",
    "#     ada_hp_selection = ada_hp_choices[0]\n",
    "#     winnow_avgd_hp_selection = winnow_hp_choices[0]\n",
    "#     ada_avgd_hp_selection = ada_hp_choices[0]\n",
    "\n",
    "    # Train models, compute test accuracy\n",
    "    \n",
    "    per_accs = []\n",
    "    avg_per_accs = []\n",
    "    winnow_accs = []\n",
    "    avg_winnow_accs = []\n",
    "    ada_accs = []\n",
    "    avg_ada_accs = []\n",
    "    svc_accs = []\n",
    "    \n",
    "    models = {0: Perceptron, 1: AveragedPerceptron,\n",
    "             2: Winnow, 3: AveragedWinnow,\n",
    "             4: AdaGrad, 5: AveragedAdaGrad}\n",
    "    \n",
    "    model_names = ['Perceptron', 'AveragedPerceptron', 'Winnow', 'AveragedWinnow', \n",
    "                  'AdaGrad', 'AveragedAdaGrad']\n",
    "    \n",
    "    accs = [per_accs, avg_per_accs, winnow_accs, avg_winnow_accs, ada_accs, avg_ada_accs, svc_accs]\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False, dtype=int)\n",
    "    \n",
    "    for size in sample_sizes:\n",
    "        x_train_slice = X_train[0:size]\n",
    "        y_train_slice = y_train[0:size]\n",
    "        \n",
    "        for i in range(0, len(models)):\n",
    "            if (i==2):\n",
    "                classifier=models[i](winnow_hp_selection, features)\n",
    "            elif (i==3):\n",
    "                classifier=models[i](winnow_avgd_hp_selection, features)\n",
    "            elif (i==4):\n",
    "                classifier=models[i](ada_hp_selection, features)\n",
    "            elif (i==5):\n",
    "                classifier=models[i](ada_avgd_hp_selection, features)\n",
    "            else:\n",
    "                classifier=models[i](features)\n",
    "            \n",
    "            classifier.train(x_train_slice, y_train_slice)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            accs[i].append(acc)\n",
    "            print(model_names[i] + ' accuracy with sample size ' + str(size) + ' is ' + str(acc))\n",
    "        \n",
    "        svc_x_train_slice = dv.fit_transform(x_train_slice)\n",
    "        svc_x_test = dv.fit_transform(X_test)\n",
    "        \n",
    "        svc = LinearSVC(loss='hinge')\n",
    "        svc.fit(svc_x_train_slice, y_train_slice)\n",
    "        svc_pred = svc.predict(svc_x_test)\n",
    "        svc_acc = accuracy_score(y_test, svc_pred)\n",
    "        accs[6].append(svc_acc)\n",
    "        print('SVC accuracy with sample size ' + str(size) + ' is ' + str(svc_acc))\n",
    "            \n",
    "        print()\n",
    "        \n",
    "    # Plot Accuracies\n",
    "    \n",
    "    plot_learning_curves(per_accs, winnow_accs, ada_accs, avg_per_accs, avg_winnow_accs, avg_ada_accs, svc_accs)\n",
    "    save_destination = 'images/' + dataset_type + '.png'\n",
    "    plt.savefig(save_destination)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = AveragedAdaGrad(1.5, sparse_features)\n",
    "# classifier.train(sparse_x_train, sparse_y_train)\n",
    "# y_pred = classifier.predict(sparse_x_test)\n",
    "# acc = accuracy_score(sparse_y_test, y_pred)\n",
    "# print(' accuracy is ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Sz_oTWqC0aSE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winnow Accuracy for Hyperparameter: 1.1 = 0.8935\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.1 = 0.939\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.01 = 0.927\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.01 = 0.8875\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.005 = 0.9195\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.005 = 0.8145\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.0005 = 0.563\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.0005 = 0.5275\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.0001 = 0.5205\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.0001 = 0.5055\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 1.5 = 0.8745\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 1.5 = 0.9265\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.25 = 0.8745\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.25 = 0.9265\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.03 = 0.8745\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.03 = 0.9265\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.005 = 0.8745\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.005 = 0.9265\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.001 = 0.8745\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.001 = 0.9265\n",
      "\n",
      "Ideal Winnow Hyperparameter: 1.01, Accuracy: 0.927\n",
      "Ideal AdaGrad Hyperparameter: 1.5, Accuracy: 0.8745\n",
      "Ideal AveragedWinnow Hyperparameter: 1.1, Accuracy: 0.939\n",
      "Ideal AveragedAdaGrad Hyperparameter: 1.5, Accuracy: 0.9265\n",
      "\n",
      "Perceptron accuracy with sample size 500 is 0.5285\n",
      "AveragedPerceptron accuracy with sample size 500 is 0.5285\n",
      "Winnow accuracy with sample size 500 is 0.5285\n",
      "AveragedWinnow accuracy with sample size 500 is 0.5285\n",
      "AdaGrad accuracy with sample size 500 is 0.5285\n",
      "AveragedAdaGrad accuracy with sample size 500 is 0.5285\n",
      "\n",
      "Perceptron accuracy with sample size 1000 is 0.6145\n",
      "AveragedPerceptron accuracy with sample size 1000 is 0.6145\n",
      "Winnow accuracy with sample size 1000 is 0.6145\n",
      "AveragedWinnow accuracy with sample size 1000 is 0.6145\n",
      "AdaGrad accuracy with sample size 1000 is 0.6145\n",
      "AveragedAdaGrad accuracy with sample size 1000 is 0.6145\n",
      "\n",
      "Perceptron accuracy with sample size 1500 is 0.6705\n",
      "AveragedPerceptron accuracy with sample size 1500 is 0.6705\n",
      "Winnow accuracy with sample size 1500 is 0.6705\n",
      "AveragedWinnow accuracy with sample size 1500 is 0.6705\n",
      "AdaGrad accuracy with sample size 1500 is 0.6705\n",
      "AveragedAdaGrad accuracy with sample size 1500 is 0.6705\n",
      "\n",
      "Perceptron accuracy with sample size 2000 is 0.709\n",
      "AveragedPerceptron accuracy with sample size 2000 is 0.709\n",
      "Winnow accuracy with sample size 2000 is 0.709\n",
      "AveragedWinnow accuracy with sample size 2000 is 0.709\n",
      "AdaGrad accuracy with sample size 2000 is 0.709\n",
      "AveragedAdaGrad accuracy with sample size 2000 is 0.709\n",
      "\n",
      "Perceptron accuracy with sample size 2500 is 0.7345\n",
      "AveragedPerceptron accuracy with sample size 2500 is 0.7345\n",
      "Winnow accuracy with sample size 2500 is 0.7345\n",
      "AveragedWinnow accuracy with sample size 2500 is 0.7345\n",
      "AdaGrad accuracy with sample size 2500 is 0.7345\n",
      "AveragedAdaGrad accuracy with sample size 2500 is 0.7345\n",
      "\n",
      "Perceptron accuracy with sample size 3000 is 0.7485\n",
      "AveragedPerceptron accuracy with sample size 3000 is 0.7485\n",
      "Winnow accuracy with sample size 3000 is 0.7485\n",
      "AveragedWinnow accuracy with sample size 3000 is 0.7485\n",
      "AdaGrad accuracy with sample size 3000 is 0.7485\n",
      "AveragedAdaGrad accuracy with sample size 3000 is 0.7485\n",
      "\n",
      "Perceptron accuracy with sample size 3500 is 0.776\n",
      "AveragedPerceptron accuracy with sample size 3500 is 0.776\n",
      "Winnow accuracy with sample size 3500 is 0.776\n",
      "AveragedWinnow accuracy with sample size 3500 is 0.776\n",
      "AdaGrad accuracy with sample size 3500 is 0.776\n",
      "AveragedAdaGrad accuracy with sample size 3500 is 0.776\n",
      "\n",
      "Perceptron accuracy with sample size 4000 is 0.796\n",
      "AveragedPerceptron accuracy with sample size 4000 is 0.796\n",
      "Winnow accuracy with sample size 4000 is 0.796\n",
      "AveragedWinnow accuracy with sample size 4000 is 0.796\n",
      "AdaGrad accuracy with sample size 4000 is 0.796\n",
      "AveragedAdaGrad accuracy with sample size 4000 is 0.796\n",
      "\n",
      "Perceptron accuracy with sample size 4500 is 0.818\n",
      "AveragedPerceptron accuracy with sample size 4500 is 0.818\n",
      "Winnow accuracy with sample size 4500 is 0.818\n",
      "AveragedWinnow accuracy with sample size 4500 is 0.818\n",
      "AdaGrad accuracy with sample size 4500 is 0.818\n",
      "AveragedAdaGrad accuracy with sample size 4500 is 0.818\n",
      "\n",
      "Perceptron accuracy with sample size 5000 is 0.8375\n",
      "AveragedPerceptron accuracy with sample size 5000 is 0.8375\n",
      "Winnow accuracy with sample size 5000 is 0.8375\n",
      "AveragedWinnow accuracy with sample size 5000 is 0.8375\n",
      "AdaGrad accuracy with sample size 5000 is 0.8375\n",
      "AveragedAdaGrad accuracy with sample size 5000 is 0.8375\n",
      "\n",
      "Perceptron accuracy with sample size 10000 is 0.9265\n",
      "AveragedPerceptron accuracy with sample size 10000 is 0.9265\n",
      "Winnow accuracy with sample size 10000 is 0.9265\n",
      "AveragedWinnow accuracy with sample size 10000 is 0.9265\n",
      "AdaGrad accuracy with sample size 10000 is 0.9265\n",
      "AveragedAdaGrad accuracy with sample size 10000 is 0.9265\n",
      "\n",
      "CPU times: user 6min 37s, sys: 172 ms, total: 6min 37s\n",
      "Wall time: 6min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\"\"\"\n",
    "Run the synthetic experiment on the sparse dataset. For reference,\n",
    "\"synthetic/sparse\" is the path to where the data is located.\n",
    "Note: This experiment takes substantial time (around 15 minutes),\n",
    "so don't worry if it's taking a long time to finish.\n",
    "\"\"\"\n",
    "run_synthetic_experiment('sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "lXcorX8ndhRo",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winnow Accuracy for Hyperparameter: 1.1 = 0.8995\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.1 = 0.9445\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.01 = 0.9215\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.01 = 0.9315\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.005 = 0.908\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.005 = 0.913\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.0005 = 0.8615\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.0005 = 0.682\n",
      "\n",
      "Winnow Accuracy for Hyperparameter: 1.0001 = 0.614\n",
      "AveragedWinnow Accuracy for Hyperparameter: 1.0001 = 0.5465\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 1.5 = 0.9325\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 1.5 = 0.9445\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.25 = 0.9325\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.25 = 0.9445\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.03 = 0.9325\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.03 = 0.9445\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.005 = 0.9325\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.005 = 0.9445\n",
      "\n",
      "AdaGrad Accuracy for Hyperparameter: 0.001 = 0.9325\n",
      "AveragedAdaGrad Accuracy for Hyperparameter: 0.001 = 0.9445\n",
      "\n",
      "Ideal Winnow Hyperparameter: 1.01, Accuracy: 0.9215\n",
      "Ideal AdaGrad Hyperparameter: 1.5, Accuracy: 0.9325\n",
      "Ideal AveragedWinnow Hyperparameter: 1.1, Accuracy: 0.9445\n",
      "Ideal AveragedAdaGrad Hyperparameter: 1.5, Accuracy: 0.9445\n",
      "\n",
      "Perceptron accuracy with sample size 500 is 0.5335\n",
      "AveragedPerceptron accuracy with sample size 500 is 0.667\n",
      "Winnow accuracy with sample size 500 is 0.799\n",
      "AveragedWinnow accuracy with sample size 500 is 0.9225\n",
      "AdaGrad accuracy with sample size 500 is 0.6205\n",
      "AveragedAdaGrad accuracy with sample size 500 is 0.7375\n",
      "SVC accuracy with sample size 500 is 0.9385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy with sample size 1000 is 0.676\n",
      "AveragedPerceptron accuracy with sample size 1000 is 0.8665\n",
      "Winnow accuracy with sample size 1000 is 0.907\n",
      "AveragedWinnow accuracy with sample size 1000 is 0.9405\n",
      "AdaGrad accuracy with sample size 1000 is 0.86\n",
      "AveragedAdaGrad accuracy with sample size 1000 is 0.9305\n",
      "SVC accuracy with sample size 1000 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 1500 is 0.8255\n",
      "AveragedPerceptron accuracy with sample size 1500 is 0.934\n",
      "Winnow accuracy with sample size 1500 is 0.8895\n",
      "AveragedWinnow accuracy with sample size 1500 is 0.9405\n",
      "AdaGrad accuracy with sample size 1500 is 0.926\n",
      "AveragedAdaGrad accuracy with sample size 1500 is 0.9405\n",
      "SVC accuracy with sample size 1500 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 2000 is 0.815\n",
      "AveragedPerceptron accuracy with sample size 2000 is 0.9405\n",
      "Winnow accuracy with sample size 2000 is 0.9205\n",
      "AveragedWinnow accuracy with sample size 2000 is 0.9405\n",
      "AdaGrad accuracy with sample size 2000 is 0.9\n",
      "AveragedAdaGrad accuracy with sample size 2000 is 0.9405\n",
      "SVC accuracy with sample size 2000 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 2500 is 0.8715\n",
      "AveragedPerceptron accuracy with sample size 2500 is 0.9405\n",
      "Winnow accuracy with sample size 2500 is 0.9255\n",
      "AveragedWinnow accuracy with sample size 2500 is 0.9405\n",
      "AdaGrad accuracy with sample size 2500 is 0.9075\n",
      "AveragedAdaGrad accuracy with sample size 2500 is 0.9405\n",
      "SVC accuracy with sample size 2500 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 3000 is 0.7985\n",
      "AveragedPerceptron accuracy with sample size 3000 is 0.9405\n",
      "Winnow accuracy with sample size 3000 is 0.93\n",
      "AveragedWinnow accuracy with sample size 3000 is 0.9405\n",
      "AdaGrad accuracy with sample size 3000 is 0.8135\n",
      "AveragedAdaGrad accuracy with sample size 3000 is 0.9405\n",
      "SVC accuracy with sample size 3000 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 3500 is 0.924\n",
      "AveragedPerceptron accuracy with sample size 3500 is 0.9405\n",
      "Winnow accuracy with sample size 3500 is 0.9245\n",
      "AveragedWinnow accuracy with sample size 3500 is 0.9405\n",
      "AdaGrad accuracy with sample size 3500 is 0.9075\n",
      "AveragedAdaGrad accuracy with sample size 3500 is 0.9405\n",
      "SVC accuracy with sample size 3500 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 4000 is 0.8195\n",
      "AveragedPerceptron accuracy with sample size 4000 is 0.9405\n",
      "Winnow accuracy with sample size 4000 is 0.9335\n",
      "AveragedWinnow accuracy with sample size 4000 is 0.9405\n",
      "AdaGrad accuracy with sample size 4000 is 0.93\n",
      "AveragedAdaGrad accuracy with sample size 4000 is 0.9405\n",
      "SVC accuracy with sample size 4000 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 4500 is 0.8915\n",
      "AveragedPerceptron accuracy with sample size 4500 is 0.9405\n",
      "Winnow accuracy with sample size 4500 is 0.9325\n",
      "AveragedWinnow accuracy with sample size 4500 is 0.9405\n",
      "AdaGrad accuracy with sample size 4500 is 0.9205\n",
      "AveragedAdaGrad accuracy with sample size 4500 is 0.9405\n",
      "SVC accuracy with sample size 4500 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 5000 is 0.896\n",
      "AveragedPerceptron accuracy with sample size 5000 is 0.9405\n",
      "Winnow accuracy with sample size 5000 is 0.932\n",
      "AveragedWinnow accuracy with sample size 5000 is 0.9405\n",
      "AdaGrad accuracy with sample size 5000 is 0.9235\n",
      "AveragedAdaGrad accuracy with sample size 5000 is 0.9405\n",
      "SVC accuracy with sample size 5000 is 0.9405\n",
      "\n",
      "Perceptron accuracy with sample size 10000 is 0.9205\n",
      "AveragedPerceptron accuracy with sample size 10000 is 0.9405\n",
      "Winnow accuracy with sample size 10000 is 0.9255\n",
      "AveragedWinnow accuracy with sample size 10000 is 0.9405\n",
      "AdaGrad accuracy with sample size 10000 is 0.9325\n",
      "AveragedAdaGrad accuracy with sample size 10000 is 0.9405\n",
      "SVC accuracy with sample size 10000 is 0.9405\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.1 s, sys: 88 ms, total: 41.2 s\n",
      "Wall time: 41.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "\"\"\"\n",
    "Run the synthetic experiment on the dense dataset. For reference,\n",
    "\"synthetic/dense\" is the path to where the data is located.\n",
    "Note: this experiment should take much less time.\n",
    "\"\"\"\n",
    "run_synthetic_experiment('dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2hLx_9Q0aSJ"
   },
   "source": [
    "##### Questions (5 points)\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Discuss the trends that you see when comparing the standard version of an algorithm to the averaged version (e.g., Winnow versus Averaged Winnow). Is there an observable trend?\n",
    "    \n",
    "    *TODO: My answer to question 1 here*\n",
    "\n",
    "\n",
    "2. We provided you with 10,000 training examples.\n",
    "   Were all 10,000 necessary to achieve the best performance for each classifier?\n",
    "   If not, how many were necessary? (Rough estimates, no exact numbers required)\n",
    "   \n",
    "   *TODO: My answer to question 2 here*\n",
    "\n",
    "\n",
    "3. Report your Final Test Accuracies\n",
    "\n",
    "  *TODO: Fill in the table*  \n",
    "\n",
    "| Model               | Sparse | Dense |\n",
    "|---------------------|--------|-------|\n",
    "| Perceptron          |        |       |\n",
    "| Winnow              |        |       |\n",
    "| AdaGrad             |        |       |\n",
    "| Averaged Perceptron |        |       |\n",
    "| Averaged Winnow     |        |       |\n",
    "| Averaged AdaGrad    |        |       |\n",
    "| SVM                 |        |       |\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPZtn5Fv0aSK"
   },
   "source": [
    "#### 3.1.5 Extra Credit (10 points)\n",
    "\n",
    "Included in the resources for this homework assignment is the code that we used to generate the synthetic data.\n",
    "We used a small amount of noise to create the dataset which you ran the experiments on.\n",
    "For extra credit, vary the amount of noise in either/both of the label and features.\n",
    "Then, plot the models' performances as a function of the amount of noise.\n",
    "Discuss your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iW_koNXv79l"
   },
   "source": [
    "TODO: Extra Credit observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMisGYXj0aSK"
   },
   "source": [
    "### 3.2 NER Experiment: Welcome to the Real World (35 points)\n",
    "\n",
    "The experiment with the NER data will analyze how changing the domain of the training and testing data can impact the performance of a model.\n",
    "\n",
    "Instead of accuracy, you will use your $F_1$ score implementation in Section 0 to evaluate how well a model does.\n",
    "Recall measures how many of the actual entities the model successfully tagged as an entity.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\textrm{Precision} &= \\frac{\\#\\textrm{(Actually Entity & Model Predicted Entity)}}{\\#\\textrm{(Model Predicted Entity)}} \\\\\n",
    "    \\textrm{Recall} &= \\frac{\\#\\textrm{(Actually Entity & Model Predicted Entity)}}{\\#\\textrm{(Actually Entity)}} \\\\\n",
    "    \\textrm{F}_1 &= 2 \\cdot \\frac{\\textrm{Precision} \\times \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For this experiment, you will only use the averaged basic Perceptron and SVM.\n",
    "Hence, no parameter tuning is necessary.\n",
    "Train both models on the CoNLL training data then compute the F$_1$ on the development and testing data of both CoNLL and Enron.\n",
    "Note that the model which is used to predict labels for Enron is trained on CoNLL data, not Enron data.\n",
    "Report the F$_1$ scores in a table.\n",
    "\n",
    "#### 3.2.1 Extracting NER Features  (25 points)\n",
    "\n",
    "Reread Section 2.2.2 to understand how to extract the features required to train the models\n",
    "and translate it to the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0gPdPmp0aSL"
   },
   "outputs": [],
   "source": [
    "def extract_ner_features_train(train):\n",
    "    \"\"\"\n",
    "    Extracts feature dictionaries and labels from the data in \"train\"\n",
    "    Additionally creates a list of all of the features which were created.\n",
    "    We have implemented the w-1 and w+1 features for you to show you how\n",
    "    to create them.\n",
    "    \n",
    "    TODO: You should add your additional featurization code here.\n",
    "    (which might require adding and/or changing existing code)\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    features = set()\n",
    "    for sentence in train:\n",
    "        padded = [('SSS', None)] + sentence[:] + [('EEE', None)]\n",
    "        for i in range(1, len(padded) - 1):\n",
    "            y.append(1 if padded[i][1] == 'I' else -1)\n",
    "            feat1 = 'w-1=' + str(padded[i - 1][0])\n",
    "            feat2 = 'w+1=' + str(padded[i + 1][0])\n",
    "            feats = [feat1, feat2]\n",
    "            features.update(feats)\n",
    "            feats = {feature: 1 for feature in feats}\n",
    "            X.append(feats)\n",
    "    return features, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQOz2fZ_0aSV"
   },
   "source": [
    "Now, repeat the process of extracting features from the test data.\n",
    "What is the difference between the code above and below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzqdP7oE0aSX"
   },
   "outputs": [],
   "source": [
    "def extract_features_dev_or_test(data, features):\n",
    "    \"\"\"\n",
    "    Extracts feature dictionaries and labels from \"data\". The only\n",
    "    features which should be computed are those in \"features\". You\n",
    "    should add your additional featurization code here.\n",
    "    \n",
    "    TODO: You should add your additional featurization code here.\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    for sentence in data:\n",
    "        padded = [('SSS', None)] + sentence[:] + [('EEE', None)]\n",
    "        for i in range(1, len(padded) - 1):\n",
    "            y.append(1 if padded[i][1] == 'I' else -1)\n",
    "            feat1 = 'w-1=' + str(padded[i - 1][0])\n",
    "            feat2 = 'w+1=' + str(padded[i + 1][0])\n",
    "            feats = [feat1, feat2]\n",
    "            feats = {feature: 1 for feature in feats if feature in features}\n",
    "            X.append(feats)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um-IG2cv0aSe"
   },
   "source": [
    "#### 3.2.2 Running the NER Experiment\n",
    "\n",
    "As stated previously, train both models on the CoNLL training data then compute the $F_1$  on the development and testing data of both CoNLL and Enron. Note that the model which is used to predict labels for Enron is trained on CoNLL data, not Enron data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxvboxmg0aSe"
   },
   "outputs": [],
   "source": [
    "def run_ner_experiment(data_path):\n",
    "    \"\"\"\n",
    "    Runs the NER experiment using the path to the ner data\n",
    "    (e.g. \"ner\" from the released resources). We have implemented\n",
    "    the standard Perceptron below. You should do the same for\n",
    "    the averaged version and the SVM.\n",
    "    \n",
    "    The SVM requires transforming the features into a different\n",
    "    format. See the end of this function for how to do that.\n",
    "    \"\"\"\n",
    "    train = load_ner_data(dataset='conll', dataset_type='train')\n",
    "    conll_test = load_ner_data(dataset='conll', dataset_type='test')\n",
    "    enron_test = load_ner_data(dataset='enron', dataset_type='test')\n",
    "\n",
    "    features, X_train, y_train = extract_ner_features_train(train)\n",
    "    X_conll_test, y_conll_test = extract_features_dev_or_test(conll_test, features)\n",
    "    X_enron_test, y_enron_test = extract_features_dev_or_test(enron_test, features)\n",
    "                 \n",
    "    # TODO: We show you how to do this for Perceptron.\n",
    "    # You should do this for the Averaged Perceptron and SVM\n",
    "    classifier = Perceptron(features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_conll_test)\n",
    "    conll_f1 = calculate_f1(y_conll_test, y_pred)\n",
    "\n",
    "    y_pred = classifier.predict(X_enron_test)\n",
    "    enron_f1 = calculate_f1(y_enron_test, y_pred)\n",
    "    print('Averaged Perceptron')\n",
    "    print('  CoNLL', conll_f1)\n",
    "    print('  Enron', enron_f1)\n",
    "    \n",
    "    # This is how you convert from the way we represent features in the\n",
    "    # Perceptron code to how you need to represent features for the SVM.\n",
    "    # You can then train with (X_train_dict, y_train) and test with\n",
    "    # (X_conll_test_dict, y_conll_test) and (X_enron_test_dict, y_enron_test)\n",
    "    vectorizer = DictVectorizer()\n",
    "    X_train_dict = vectorizer.fit_transform(X_train)\n",
    "    X_conll_test_dict = vectorizer.transform(X_conll_test)\n",
    "    X_enron_test_dict = vectorizer.transform(X_enron_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yy6Y9320aSi"
   },
   "outputs": [],
   "source": [
    "# Run the NER experiment. \"ner\" is the path to where the data is located.\n",
    "run_ner_experiment('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeElQnS_0aSw"
   },
   "source": [
    "\n",
    "##### F1 Scores Table \n",
    "TODO: report your values:  \n",
    "\n",
    "| Model               | CoNLL Test F1 | Enron Test F1 |\n",
    "|---------------------|---------------|---------------|\n",
    "| Averaged Perceptron |               |               |\n",
    "| SVM                 |               |               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpzm2yA20aSx"
   },
   "source": [
    "##### Questions (5 points)\n",
    "\n",
    "Comment on the results:\n",
    "1. Are the F$_1$ scores on CoNLL and Enron similar?\n",
    "    \n",
    "    *TODO: My answer to question 1 here*\n",
    "    \n",
    "\n",
    "2. If they are dissimilar, explain why you think the F$_1$ score increased/decreased on the Enron data.\n",
    "\n",
    "    *TODO: My answer to question 2 here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKr_PXhO0aSy"
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "We will be using Gradescope to turn in both the Python code.\n",
    "You should have been automatically added to Gradescope.\n",
    "If you do not have access, please ask the TA staff on Piazza.\n",
    "\n",
    "There are three parts to the submission on Gradescope:\n",
    "* ipynb file: Submit this notebook (.ipynb). If you are using Google Colab, you will have to download it as an ipynb file.\n",
    "* PDF: This notebook saved as a PDF. We will use this to see the overall structure of your homework and code, and to check manual questions like the tables, plots, and your discussions. How should I convert this notebook to PDF, you may ask? There are a few ways, but for simplicity print the Jupyter notebook and _Save as PDF_.\n",
    "* Code: A `hw2.py` file. We will use this to unit test and automate grading some parts of the homework. We only need a few functions/classes from the whole notebook. In particular, to unit test we only need:\n",
    "    - `calculate_f1`\n",
    "    - `highest_and_lowest_f1_score`\n",
    "    - `Perceptron`, `Winnow`, and `AdaGrad` classes.\n",
    "    \n",
    "There are two ways to submit these pieces of code. You can either manually copy and paste these to a Python file,\n",
    "or you can use Jupyter's _Download as .py_, and delete all unnecessary code. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
